{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c67f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)  \n",
    "\n",
    "%run functions.py\n",
    "\n",
    "# load dataset\n",
    "df_new = pd.read_csv('dataset_with_all_vars_converted_to_numerical_values.csv')\n",
    "# determine the outcome label \n",
    "col_y='sab'\n",
    "# drop variables we do not use in static models\n",
    "df_new = df_drop(df_new, ['startweek', 'censorweek'])\n",
    "df_new.shape\n",
    "\n",
    "# do the needed preparation\n",
    "## Center the “number of pregnancies” variable at the median for the gravid cohort\n",
    "median_numpregs = df_new['b_numpregs'].median()\n",
    "temp_numpregs = df_new.loc[(df_new['b_everpregnant']==1), 'b_numpregs']\n",
    "df_new.loc[(df_new['b_everpregnant']==1), 'b_numpregs'] = temp_numpregs - median_numpregs\n",
    "\n",
    "# Statistical Feature Selection (SFS) \n",
    "# step 1\n",
    "# drop variables wit std of zero\n",
    "df_std = df_new.std()\n",
    "drop_cols = df_std[df_std<0.001].index.values\n",
    "df_new = df_drop(df_new, drop_cols)\n",
    "\n",
    "# compute the variables statistics and save it in dataframe named \"result\"\n",
    "y = df_new[col_y].astype(int)\n",
    "result = stat_test(df_new, y)\n",
    "\n",
    "# step 2\n",
    "# Compute pairwise correlation of variables with each other and determine pairs with corr > 0.9\n",
    "# Compute the correlation of highly correlated pairs with outcome as well as their p-value\n",
    "# We remove one variable among highly correlated pairs\n",
    "selected_col_to_remove, df_highcorr = highcorr_stats(df_new, col_y, thres=0.9)\n",
    "df_new = df_drop(df_new, selected_col_to_remove)\n",
    "\n",
    "# step 3\n",
    "# We test the association between each variable and the outcome \n",
    "# Chi2 test is used for non-continuous variables, and KS test is used for continuous variables\n",
    "# We remove variables that are not independently associated with the outcome based on p-value > 0.05\n",
    "drop_cols = result.loc[result['p-value']>0.05,'Variable'].values\n",
    "df_new = df_drop(df_new, drop_cols)\n",
    "\n",
    "\n",
    "# STATIC MODEL DEVELOPMENT \n",
    "cols_rep=['AUC', 'Accuracy','weighted F1-score','weighted_precision_score','weighted_recall_score']# metrics we are interested in\n",
    "my_scoring='roc_auc' # scoring metric for GridSearchCV\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "# FULL MODEL\n",
    "# Logistic Regression with l1 norm regularization \n",
    "df_coef_,metrics_df = tr_predict(df_new, col_y=col_y,target_names = ['0', '1'], model='LR',penalty='l1',cv_folds=5,scoring=my_scoring)\n",
    "df_AUCs=metrics_df.rename(index={0: 'LR-L1'})\n",
    "result__=df_coef_.merge(result,how='left', on='Variable')\n",
    "print(result__[['Variable','coef_','y_corr','p-value','y1_mean', 'y0_mean', 'All_mean', 'All_std']])\n",
    "# Support Vector Machines with l1 norm regularization\n",
    "df_coef_,metrics_df = tr_predict(df_new, col_y=col_y,target_names = ['0', '1'], model='SVM',penalty='l1',cv_folds=5,scoring=my_scoring)\n",
    "df_AUCs=pd.concat([df_AUCs,metrics_df.rename(index={0: 'SVM-L1'})])\n",
    "# Gradient Boosted Decision Trees, Light Gradient Boosting Machine\n",
    "df_coef_,metrics_df = tr_predict(df_new, col_y=col_y,target_names = ['0', '1'], model='LGB',penalty='l1',cv_folds=5,scoring=my_scoring)\n",
    "df_AUCs=pd.concat([df_AUCs,metrics_df.rename(index={0: 'GBT'})])\n",
    "# Random Forest\n",
    "df_coef_,metrics_df = tr_predict(df_new, col_y=col_y,target_names = ['0', '1'], model='RF',penalty='l1',cv_folds=5,scoring=my_scoring)\n",
    "df_AUCs=pd.concat([df_AUCs,metrics_df.rename(index={0: 'RF'})])\n",
    "print(df_AUCs)\n",
    "\n",
    "# SPARSE MODEL\n",
    "# Feature_selection by RFE\n",
    "names = df_new.drop(col_y, axis=1).columns\n",
    "num_of_cols = len(df_new.columns)\n",
    "from sklearn.feature_selection import RFE,RFECV\n",
    "metric_all_rfe = []\n",
    "Xraw = df_new.drop(col_y, axis=1).values\n",
    "my_range = range(1,num_of_cols)\n",
    "my_penalty = 'l1'\n",
    "for my_C in [0.1, 1]: # try different hyperparameter C with the LR model which we use as the estimator in RFE\n",
    "    for n_select in my_range: # try different numbers of features to find how many features result in best performance\n",
    "        # Standardize features by removing the mean and scaling to unit variance\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        # Fits transformer to X and returns a transformed version of X\n",
    "        X = scaler.fit_transform(Xraw)\n",
    "        # the LR model which we use as the estimator in RFE\n",
    "        clf = LogisticRegression(C=my_C, penalty=my_penalty, tol=0.01, class_weight='balanced', solver='liblinear')#0.\n",
    "        # select features by recursively considering smaller and smaller sets of features\n",
    "        rfe = RFE(estimator= clf, n_features_to_select=n_select, step=1)\n",
    "        rfe.fit(X, y.ravel())\n",
    "        # Selected (i.e., estimated best) features are assigned rank 1\n",
    "        # so we drop features ranked greater than 1\n",
    "        X=df_new.drop(names[rfe.ranking_>1], axis=1)\n",
    "         # evaluate the dataset of selected features using 'LR' model with 'l2' norm regularization\n",
    "        df_coef_RFE, metric_df_RFE=tr_predict(X, col_y=col_y, target_names = ['0', '1'], model='LR',penalty='l2',cv_folds=5,scoring=my_scoring)\n",
    "        metric_all_rfe.append([my_C, n_select]+metric_df_RFE.values.tolist()[0])\n",
    "metric_all_rfe = pd.DataFrame(metric_all_rfe, columns=['my_C','n_select','AUC-mean','AUC-std','Accuracy-mean','Accuracy-std','weighted_F1_score-mean','weighted_F1_score-std','weighted_precision_score-mean','weighted_precision_score-std','weighted_recall_score-mean','weighted_recall_score-std'])\n",
    "# we pick the my_C and n_select that lead to the model with highest 'AUC-mean' minus 'AUC-std' \n",
    "metric_all_rfe['AUC_'] = metric_all_rfe['AUC-mean'] - metric_all_rfe['AUC-std']\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X = scaler.fit_transform(Xraw)\n",
    "clf = LogisticRegression(C=metric_all_rfe.loc[metric_all_rfe['AUC_'].idxmax(),'my_C'], penalty=my_penalty, tol=0.01, class_weight='balanced', solver='liblinear')#0.\n",
    "rfe = RFE(estimator=clf, n_features_to_select=metric_all_rfe.loc[metric_all_rfe['AUC_'].idxmax(),'n_select'], step=1)\n",
    "rfe.fit(X, y.ravel())\n",
    "X = df_new.drop(names[rfe.ranking_>1], axis=1)# our dataframe after featture selection by recursive feature elimination\n",
    "\n",
    "# Logistic Regression with l2 norm regularization\n",
    "df_coef_, metrics_df_=tr_predict(X, col_y=col_y, target_names = ['0', '1'], model='LR',penalty='l2',cv_folds=5,scoring=my_scoring)\n",
    "result_RFE=df_coef_.merge(result,how='left', on='Variable')\n",
    "print(result_RFE[['Variable','coef_','y_corr','p-value','y1_mean','y0_mean','All_mean','All_std']])\n",
    "print(metrics_df_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
